
# Mixed Effects Model with Poisson Likelihood {#pois-rep}

In the previous section, the observed data $Y$ was a matrix of directed rankings of nodes, where $Y_{i,j}$ denoted the ranking of $j$ by $i$. The analysis performed took into count their relative sizes. In other words, given that we observed actor $i$ ranking actor $j$ higher than actor $k$, our inferred relationship would retain this ranking. However, what if instead treated these rankings as count data? In other words, say our matrix $Y$ represented a weighted, directed graph of emails, where $Y_{i,j}$ denoted the number of emails actor $i$ sent to actor $j$. This new treatment of the data would purely focus on the magnitudes of the relationships between the actors, and not their relative rankings.

## Poisson model with log-link

We can model the number of emails sent from actor $i$ to actor $j$ as a Poisson distributed variable with mean $e^{\theta_{i,j}}$. Hoff's paper defines a log-link function to relate the regression model to the Poisson nature of the data. We first perform linear regression on $\theta_{i,j}$, where

$\theta_{i,j}=\beta_{d1}x_{i,j,1}+\beta_{d2}x_{i,j,2}+a_i+b_j+\epsilon_{i,j}$

$Y_{i,j}\sim Pois(e^{\theta_{i,j}})$

For the code replication we temporarily assume there is no correlation between entries in $Y$. Therefore, $\epsilon_{i,j}=0$. This assumption is likely not true with real data, as we almost always expect to observe some sort of correlation between relationships.

With the use of the log-link, our posterior estimate for $\theta$ is not conjugate. Therefore, we must perform a Metropolis Hastings step to accept/reject proposed values of $\theta$. The step used in Hoff's paper is described below. Note how the proposal distribution is symmetric, which means this is simply a Metropolis step.

Proposal distribution for $\theta_{i,j}$: $\theta^*_{i,j} \sim N(\beta_{d1}x_{i,j,1}+\beta_{d2}x_{i,j,2}+a_i+b_j, 1)$ $\forall i\neq j$

Accept $\theta_{i,j}$ with probability $\dfrac{p(y_{i,j}|\theta^*_{i,j})p(\theta^*_{i,j}|y_{i,j})}{p(y_{i,j}|\theta_{i,j})p(\theta_{i,j}|y_{i,j})}$

## Gibbs Sampler
The process of the Gibbs sampler conducted in Hoff's paper is outlined below. Note how similar it is to the Gibbs sampler under the FRN likelihood. The only change to the process is the calculation of the likelihood. All other parameter estimations remain the same.

1. Sample $\beta_1,\beta_2$ from full conditional.
2. Sample $a,b$ from full conditional.
3. for each $i\neq j$
    - propose $\theta_{i,j}$ and accept/reject

## Code replication
The code for the Gibbs sampler was replicated from scratch with a simulated dataset $Y$ and a simulated covariance matrix $X$. The observed data were generated as follows.

$y_{i,j} \sim Pois(4)$ $\forall i\neq j$

$x_{i,j,1}=1$ $\forall i\neq j$

$x_{i,j,2}=1$ $\forall i\leq \frac{n}{2}$

$x_{i,j,2}=1$ $\forall i> \frac{n}{2}$

The first dyadic coefficient acts as an intercept to the model, hence the lack of a need for a separate coefficient for the intercept in our regression model. The second dyadic coefficient acts as a kind of group membership variable, where the first 50 rows are part of the first group ($X_{i,j,2}=1$) and the last 50 rows are part of the second group ($X_{i,j,2}=0$).

In order to test for the accuracy of the replicated results, the posterior parameter estimates were compared to results from the "AMEN" package, an R package created by Hoff and Volfovsky to run mixed effects models on network data using Gibbs samplers. Below are the results of the replicated code and the "AMEN" package's $ame()$ function when given the same model and data.

```{r,echo=FALSE, eval=TRUE, include=FALSE}
library(MASS)
library(MCMCpack)
library(amen)
```

```{r,echo=FALSE, eval=TRUE}
#Specify example dataset
n=100
p=2
m=n-50
Z<-matrix(nrow=n,ncol=n)
Y<-matrix(nrow=n,ncol=n)
diag(Y)<--1
for(i in seq(1,n,1)){
  for(j in seq(1,n,1)){
    if(i!=j){
      Y[i,j]<-rpois(n=1,lambda=4)
    }
  }
}
X<-array(dim=c(n,n,p))
X[,,1]<-matrix(1,nrow=n,ncol=n)
diag(X[,,1])<--1
A<-matrix(1,nrow=n/2,ncol=n/2)
B<-matrix(0,nrow=n/2,ncol=n/2)
X[,,2]<-rbind(A,B)
diag(X[,,2])<--1
#Specify prior parameters
beta_0<-rep(0,p)
Sigma_0<-matrix(nrow=p,ncol=p)
Sigma_0[1,]<-c(2,1)
Sigma_0[2,]<-c(1,2)
v_0<-4
S_0<-matrix(nrow=2,ncol=2)
S_0[1,]<-c(2,1)
S_0[2,]<-c(1,2)
I2<-matrix(0,nrow=n-1,ncol=n-1)
diag(I2)<-1/6

#Set starting values
beta<-mvrnorm(n=1,mu=beta_0,Sigma=Sigma_0)
Sigma_ab<-riwish(v_0,solve(S_0))
a<-c()
b<-c()
for(i in seq(1,n,1)){
  ab<-mvrnorm(n=1,mu=c(0,0),Sigma=Sigma_ab)
  a<-c(a,ab[1])
  b<-c(b,ab[2])
}
Theta<-matrix(0,nrow=n,ncol=n)
diag(Theta)<--1
for(i in seq(1,n,1)){
  for(j in seq(1,n,1)){
    if(i!=j){
      Theta[i,j]<-beta%*%X[i,j,]+a[i]+b[j]
    }
  }
}
#Create matrices to store values
s=1000
betas<-matrix(nrow=s,ncol=p)
rs<-c()
```

```{r,echo=FALSE, eval=TRUE}
#Start Gibbs Sampler
for(k in seq(1,s,1)){

#Sample Sigma_ab from posterior
  Sigma_ab<-riwish(v_0+n,solve(S_0)+t(cbind(a,b))%*%cbind(a,b))
  sigma_a<-sqrt(Sigma_ab[1,1])
  sigma_b<-sqrt(Sigma_ab[2,2])
    
#Sample a from posterior
  mu<-c()
  for(i in seq(1,n,1)){
    Theta2<-c()
    for(j in seq(1,n,1)){
      if(i!=j){
      Theta2<-c(Theta2,Theta[i,j]-t(beta)%*%X[i,j,]-b[j])
      }
    }
    mu<-c(mu,(sum(Theta2[-i])/(1/sigma_a^2+n-1)))
  }
    temp<-matrix(0,nrow=n,ncol=n)
    diag(temp)<-solve(1/sigma_a^2+n-1)
    a<-mvrnorm(n=1,mu=mu,Sigma=temp)
  
#Sample b from posterior
  mu<-c()
  for(i in seq(1,n,1)){
    Theta2<-c()
    for(j in seq(1,n,1)){
      if(i!=j){
      Theta2<-c(Theta2,Theta[j,i]-t(beta)%*%X[j,i,]-a[i])
      }
    }
    mu<-c(mu,(sum(Theta2[-j])/(1/sigma_b^2+n-1)))
  }
    temp<-matrix(0,nrow=n,ncol=n)
    diag(temp)<-solve(1/sigma_b^2+n-1)
    a<-mvrnorm(n=1,mu=mu,Sigma=temp)
  
  
#Sample beta from full conditional
  F1<-matrix(nrow=n^2-n,ncol=1)
  F1[,1]<-Theta[Theta!=-1]
  G<-matrix(nrow=n^2-n,ncol=2)
  H<-X[,,1]
  G[,1]<-H[H!=-1]
  H<-X[,,2]
  G[,2]<-H[H!=-1]
  beta_mu<-solve(solve(Sigma_0)+t(G)%*%G)%*%(solve(Sigma_0)%*%beta_0+t(G)%*%F1)
  beta_Sigma<-solve(solve(Sigma_0)+t(G)%*%G)
  
  beta<-mvrnorm(n=1,mu=beta_mu,Sigma=beta_Sigma)
  
  #Propose Thetas
  for(i in seq(1,n,1)){
    Xi<-X[i,,]
    Xi<-Xi[-i,]
    Yi<-Y[i,]
    Yi<-Yi[-i]
    Thetai<-Theta[i,]
        Tprop<-mvrnorm(n=1,mu=Xi%*%beta+a[i]+b[-i],Sigma=I2)
        logr<-sum(dpois(Yi,exp(Tprop),log=T))-
          sum(dpois(Yi,exp(Thetai),log=T))+
          sum(dnorm(Tprop,mean=Xi%*%beta+a[i]+b[-i],sd=sqrt(1/6),log=T))-
          sum(dnorm(Thetai,mean=Xi%*%beta+a[i]+b[-i],sd=sqrt(1/6),log=T))
        if(log(runif(1))<logr){
          Tprop<-append(Tprop,-1,after=i-1)
          Theta[i,]<-Tprop
          rs<-c(rs,1)
        }else{
          rs<-c(rs,0)
        }
  }
  
#Store beta
  betas[k,]<-beta
}
```

```{r,echo=FALSE, eval=TRUE}
print("Acceptance Rate")
print(mean(rs))
print("Posterior estimate for beta")
for(i in seq(1,p,1)){
  print(mean(betas[,i]))
}
print("Plots")
A<-seq(1,s,1)
plot(A,betas[,1],type="l", xlab="Iteration",ylab="Beta_1",main="Posterior Estimates from Gibbs Sampler for Beta_1")
plot(A,betas[,2],type="l", xlab="Iteration",ylab="Beta_2",main="Posterior Estimates from Gibbs Sampler for Beta_2")
acf(betas[,1],type="correlation",main="ACF for Beta_1")
acf(betas[,2],type="correlation",main="ACF for Beta_2")
```

