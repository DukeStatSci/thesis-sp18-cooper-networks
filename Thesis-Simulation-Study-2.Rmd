```{r include_packages_2, include = FALSE}
if(!require(devtools))
  install.packages("devtools", repos = "http://cran.rstudio.com")
if(!require(dplyr))
    install.packages("dplyr", repos = "http://cran.rstudio.com")
if(!require(ggplot2))
    install.packages("ggplot2", repos = "http://cran.rstudio.com")
if(!require(ggplot2))
    install.packages("bookdown", repos = "http://cran.rstudio.com")
if(!require(thesisdowndss)){
  library(devtools)
  devtools::install_github("mine-cetinkaya-rundel/thesisdowndss")
  }
library(thesisdowndss)
```

# Simulation Study {#sim-study}

Now that we've been able to replicate the code for simulating with a fixed-rank likelihood approach as well as the code for simulating with a poisson likelihood approach, we can compare the two methods in their accuracy of estimating row, column and dyadic coefficients. To do this we conducted a simulation study. We first established the true parameters to be some specific values to judge our estimates by. For each simulation, we generated data from the true model and estimated the posterior coefficients using the two approaches. By using the same generated data for each approach we can directly compare their ability to capute the row, column, and dyadic effects of the model. First we generated count data $Y$ to use for the Poisson likelihood approach. Then we "binarized" this data to create the data frame $Y^b$ where

$Y^b_{i,j}=1$ if $Y_{i,j}>0$

$Y^b_{i,j}=0$ if $Y_{i,j}\leq0$

and used this data frame for our estimation using the binary likelihood approach.

Our generation of the data comes from the approach used in section 3.2 from Hoff et. al 2013.

##True Parameters

Our model for $Y$ is

$Y_{i,j}=\beta_{d1}X_{i,j,1}+\beta_{d2}X_{i,j,2}+\beta_{r}X_{i,r}+\beta_{c}X_{j,c}+a_i+b_j+\epsilon_{i,j}$

For our simulation we generate two dyadic covariates in addition to a row and column-specific covariate. Note that for this simulation we also had zero correlation between entries in $Y$, so $\epsilon_{i,j}=0$ for all $i$, $j$. This assumption likely can't be made in the context of real data, as some form of correlation almost always exists between entries in $Y$.

Our true values for the coefficients are given below.

$\beta_{d1}=\beta_{d2}=\beta_r=\beta_c=1$

$\Sigma_{ab}= \begin{bmatrix} 1 & 0.5 \\ 0.5 & 1 \end{bmatrix}$


##Generating Data

Our generated data comes from sampling from standard normal distributions. Here we have a matrix of n=100 actors/nodes.

$x_{1,r},x_{2,r},...,x_{n,r} \sim$ i.i.d. $N(0,1)$

$x_{1,c},x_{2,c},...,x_{n,c} \sim$ i.i.d. $N(0,1)$

$x_{i,j,1} \sim$ i.d.d. $N(0,1)$ $\forall i!=j$

$x_{i,j,2}=\dfrac{z_iz_j}{0.42}$, where $z_1,z_2,...,z_n \sim$ i.i.d. $Bern(0.5)$

Note here the slightly different method for generating the second dyad covariate. Here the second dyad is meant to simulate a kind of group membership.

##Results
Eight simulations were conducted in total. The Gibbs samplers for both the Poisson Likelihood approach and the Binary Likelihood approach were run for 10,000 simulations each. The posterior estimates for each \Beta are plotted in the figures below.

```{r fig1, results="asis", echo=FALSE, fig.cap="Figure 1: Posterior Estimates for Beta_1", out.extra="scale=0.75"}
include_graphics("figure/Beta1Plot.png")
```

```{r fig2, results="asis", echo=FALSE, fig.cap="Figure 1: Posterior Estimates for Beta_2", out.extra="scale=0.75"}
include_graphics("figure/Beta2Plot.png")
```

```{r fig3, results="asis", echo=FALSE, fig.cap="Figure 1: Posterior Estimates for Beta_3", out.extra="scale=0.75"}
include_graphics("figure/Beta3Plot.png")
```

```{r fig4, results="asis", echo=FALSE, fig.cap="Figure 1: Posterior Estimates for Beta_4", out.extra="scale=0.75"}
include_graphics("figure/Beta4Plot.png")
```




```{r setup, include=FALSE, eval=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(MCMCpack)
library(truncnorm)
library(ggplot2)
```

```{r echo=FALSE, eval=FALSE}
#True parameters
Beta_row_true=Beta_col=Beta_dyad1=Beta_dyad2=1
Sigma_ab_true=matrix(c(1,.5,.5,1),nrow=2,ncol=2)
#Sigma_e_true=matrix(c(1,.9,.9,1),nrow=2,ncol=2)
s<-100
num_sim<-3
Store<-data.frame(Est=1,Beta=-1,Type=factor("Poisson",level=c("Poisson","Binary")),Sim=4)
```


```{r echo=FALSE, eval=FALSE}
for(q in seq(1,num_sim,1)){
#Simulate X
n=100
p=4
X<-array(dim=c(n,n,p))
X_row<-rnorm(n=n,mean=0,sd=1)
X_col<-rnorm(n=n,mean=0,sd=1)
for(i in seq(1,n,1)){
  X[i,-i,3]<-rep(X_row[i],n-1)
  X[-i,i,4]<-rep(X_col[i],n-1)
}
X[,,1]<-rnorm(n=n^2,mean=0,sd=1)
Z<-matrix(nrow=n,ncol=n)
Z[,]<-rbinom(n=n^2,size=1,prob=.5)
X[,,2]<-Z[,]/.42
diag(X[,,1])<--1
diag(X[,,2])<--1
diag(X[,,3])<--1
diag(X[,,4])<--1

ab<-mvrnorm(n=n,mu=c(0,0),Sigma=Sigma_ab_true)
Y<-matrix(nrow=n,ncol=n)
for(i in seq(1,n,1)){
  for(j in seq(1,n,1)){
    if(i!=j){
      #ep<-mvrnorm(mu=c(0,0),Sigma=Sigma_e_true)
      Y[i,j]<-rpois(1,exp(sum(X[i,j,])+ab[i,1]+ab[j,2]))
    }
  }
}
S<-matrix(0,nrow=n,ncol=n)
for(i in seq(1,n,1)){
  for(j in seq(1,n,1)){
    if(i!=j){
      if(Y[i,j]>0){
        S[i,j]<-1
      }
    }
  }
}
diag(S)<--1
#Specify prior parameters
beta_0<-rep(0,p)
Sigma_0<-matrix(nrow=p,ncol=p)
Sigma_0[1,]<-c(2,1,1,1)
Sigma_0[2,]<-c(1,2,1,1)
Sigma_0[3,]<-c(1,1,2,1)
Sigma_0[4,]<-c(1,1,1,2)
v_0<-4
S_0<-matrix(nrow=2,ncol=2)
S_0[1,]<-c(2,1)
S_0[2,]<-c(1,2)
I2<-matrix(0,nrow=n-1,ncol=n-1)
diag(I2)<-1/6

#Set starting values
beta<-mvrnorm(n=1,mu=beta_0,Sigma=Sigma_0)
Sigma_ab<-riwish(v_0,solve(S_0))
a<-c()
b<-c()
for(i in seq(1,n,1)){
  ab<-mvrnorm(n=1,mu=c(0,0),Sigma=Sigma_ab)
  a<-c(a,ab[1])
  b<-c(b,ab[2])
}
Theta<-matrix(0,nrow=n,ncol=n)
diag(Theta)<--1
for(i in seq(1,n,1)){
  for(j in seq(1,n,1)){
    if(i!=j){
      Theta[i,j]<-beta%*%X[i,j,]+a[i]+b[j]
    }
  }
}
betas_poisson<-matrix(nrow=s,ncol=p)
rs_poisson<-c()

#Start Gibbs Sampler for Count Data
for(k in seq(1,s,1)){

#Sample Sigma_ab from posterior
  Sigma_ab<-riwish(v_0+n,solve(S_0)+t(cbind(a,b))%*%cbind(a,b))
  sigma_a<-sqrt(Sigma_ab[1,1])
  sigma_b<-sqrt(Sigma_ab[2,2])
    
#Sample a from posterior
  mu<-c()
  for(i in seq(1,n,1)){
    Theta2<-c()
    for(j in seq(1,n,1)){
      if(i!=j){
      Theta2<-c(Theta2,Theta[i,j]-t(beta)%*%X[i,j,]-b[j])
      }
    }
    mu<-c(mu,(sum(Theta2[-i])/(1/sigma_a^2+n-1)))
  }
    temp<-matrix(0,nrow=n,ncol=n)
    diag(temp)<-solve(1/sigma_a^2+n-1)
    a<-mvrnorm(n=1,mu=mu,Sigma=temp)
  
#Sample b from posterior
  mu<-c()
  for(i in seq(1,n,1)){
    Theta2<-c()
    for(j in seq(1,n,1)){
      if(i!=j){
      Theta2<-c(Theta2,Theta[j,i]-t(beta)%*%X[j,i,]-a[i])
      }
    }
    mu<-c(mu,(sum(Theta2[-j])/(1/sigma_b^2+n-1)))
  }
    temp<-matrix(0,nrow=n,ncol=n)
    diag(temp)<-solve(1/sigma_b^2+n-1)
    a<-mvrnorm(n=1,mu=mu,Sigma=temp)
  
  
#Sample beta from full conditional
  F1<-matrix(nrow=n^2-n,ncol=1)
  F1[,1]<-Theta[Theta!=-1]
  G<-matrix(nrow=n^2-n,ncol=4)
  H<-X[,,1]
  G[,1]<-H[H!=-1]
  H<-X[,,2]
  G[,2]<-H[H!=-1]
  H<-X[,,3]
  G[,3]<-H[H!=-1]
  H<-X[,,4]
  G[,4]<-H[H!=-1]
  beta_mu<-solve(solve(Sigma_0)+t(G)%*%G)%*%(solve(Sigma_0)%*%beta_0+t(G)%*%F1)
  beta_Sigma<-solve(solve(Sigma_0)+t(G)%*%G)
  
  beta<-mvrnorm(n=1,mu=beta_mu,Sigma=beta_Sigma)
  
  #Propose Thetas
  for(i in seq(1,n,1)){
    Xi<-X[i,,]
    Xi<-Xi[-i,]
    Yi<-Y[i,]
    Yi<-Yi[-i]
    Thetai<-Theta[i,]
    Thetai<-Thetai[-i]
        Tprop<-mvrnorm(n=1,mu=Xi%*%beta+a[i]+b[-i],Sigma=I2)
        logr<-sum(dpois(Yi,exp(Tprop),log=T))-
          sum(dpois(Yi,exp(Thetai),log=T))+
          sum(dnorm(Tprop,mean=Xi%*%beta+a[i]+b[-i],sd=sqrt(1/6),log=T))-
          sum(dnorm(Thetai,mean=Xi%*%beta+a[i]+b[-i],sd=sqrt(1/6),log=T))
        if(log(runif(1))<logr){
          Tprop<-append(Tprop,-1,after=i-1)
          Theta[i,]<-Tprop
          rs_poisson<-c(rs_poisson,1)
        }else{
          rs_poisson<-c(rs_poisson,0)
        }
  }
  
#Store beta
  betas_poisson[k,]<-beta
}

#Specify prior parameters
beta_0<-rep(0,p)
Sigma_0<-matrix(nrow=p,ncol=p)
Sigma_0[1,]<-c(2,1,1,1)
Sigma_0[2,]<-c(1,2,1,1)
Sigma_0[3,]<-c(1,1,2,1)
Sigma_0[4,]<-c(1,1,1,2)
v_0<-4
S_0<-matrix(nrow=2,ncol=2)
S_0[1,]<-c(2,1)
S_0[2,]<-c(1,2)
I2<-matrix(0,nrow=n-1,ncol=n-1)
diag(I2)<-1/6

#Set starting values
beta<-mvrnorm(n=1,mu=beta_0,Sigma=Sigma_0)
Sigma_ab<-riwish(v_0,solve(S_0))
a<-c()
b<-c()
for(i in seq(1,n,1)){
  ab<-mvrnorm(n=1,mu=c(0,0),Sigma=Sigma_ab)
  a<-c(a,ab[1])
  b<-c(b,ab[2])
}
betas_binary<-matrix(nrow=s,ncol=p)
rs_binary<-c()
Z<-S

#Start Gibbs Sampler for Count Data
for(k in seq(1,s,1)){

#Sample Sigma_ab from posterior
  Sigma_ab<-riwish(v_0+n,solve(S_0)+t(cbind(a,b))%*%cbind(a,b))
  sigma_a<-sqrt(Sigma_ab[1,1])
  sigma_b<-sqrt(Sigma_ab[2,2])
    
#Sample a from posterior
  mu<-c()
  for(i in seq(1,n,1)){
    Z2<-c()
    for(j in seq(1,n,1)){
      if(i!=j){
      Z2<-c(Z2,Z[i,j]-t(beta)%*%X[i,j,]-b[j])
      }
    }
    mu<-c(mu,(sum(Z2[-i])/(1/sigma_a^2+n-1)))
  }
    temp<-matrix(0,nrow=n,ncol=n)
    diag(temp)<-solve(1/sigma_a^2+n-1)
    a<-mvrnorm(n=1,mu=mu,Sigma=temp)
  
#Sample b from posterior
  mu<-c()
  for(i in seq(1,n,1)){
    Z2<-c()
    for(j in seq(1,n,1)){
      if(i!=j){
      Z2<-c(Z2,Z[j,i]-t(beta)%*%X[j,i,]-a[i])
      }
    }
    mu<-c(mu,(sum(Z2[-j])/(1/sigma_b^2+n-1)))
  }
    temp<-matrix(0,nrow=n,ncol=n)
    diag(temp)<-solve(1/sigma_b^2+n-1)
    a<-mvrnorm(n=1,mu=mu,Sigma=temp)
  
  
#Sample beta from full conditional
  F1<-matrix(nrow=n^2-n,ncol=1)
  F1[,1]<-Z[Z!=-1]
  G<-matrix(nrow=n^2-n,ncol=4)
  H<-X[,,1]
  G[,1]<-H[H!=-1]
  H<-X[,,2]
  G[,2]<-H[H!=-1]
  H<-X[,,3]
  G[,3]<-H[H!=-1]
  H<-X[,,4]
  G[,4]<-H[H!=-1]
  beta_mu<-solve(solve(Sigma_0)+t(G)%*%G)%*%(solve(Sigma_0)%*%beta_0+t(G)%*%F1)
  beta_Sigma<-solve(solve(Sigma_0)+t(G)%*%G)
  
  beta<-mvrnorm(n=1,mu=beta_mu,Sigma=beta_Sigma)
  
  for(i in seq(1,n,1)){
    for(j in seq(1,n,1)){
      if(i!=j){
        if(S[i,j]>0){
          Z[i,j]<-rtruncnorm(n=1,a=0,b=Inf,mean=X[i,j,]%*%beta+a[i]+b[j],sd=1)
        }else{
          Z[i,j]<-rtruncnorm(n=1,a=-Inf,b=0,mean=X[i,j,]%*%beta+a[i]+b[j],sd=1)
        }
      }
    }
  }
#Store beta
betas_binary[k,]<-beta
}

for(w in seq(1,p,1)){
  Store<-rbind(Store,c(mean(betas_poisson[,w]),w,"Poisson",q))
  Store<-rbind(Store,c(mean(betas_binary[,w]),w,"Binary",q))
}
}
```

```{r echo=FALSE, eval=FALSE}
ggplot(data=Store[Store$Beta==1,], aes(x=Sim, y=as.numeric(Est), group=Type, color=Type)) +
    geom_line() +
    geom_point() + geom_hline(yintercept = 1) + labs(colour = "Likelihood", x="Simulation", y="Beta_1 Estimate", title="Simulated Posterior Estimates for Beta_1")

ggplot(data=Store[Store$Beta==2,], aes(x=Sim, y=as.numeric(Est), group=Type, color=Type)) +
    geom_line() +
    geom_point() + geom_hline(yintercept = 1) + labs(colour = "Likelihood", x="Simulation", y="Beta_2 Estimate", title="Simulated Posterior Estimates for Beta_2")

ggplot(data=Store[Store$Beta==3,], aes(x=Sim, y=as.numeric(Est), group=Type, color=Type)) +
    geom_line() +
    geom_point() + geom_hline(yintercept = 1) + labs(colour = "Likelihood", x="Simulation", y="Beta_3 Estimate", title="Simulated Posterior Estimates for Beta_3")

ggplot(data=Store[Store$Beta==4,], aes(x=Sim, y=as.numeric(Est), group=Type, color=Type)) +
    geom_line() +
    geom_point() + geom_hline(yintercept = 1) + labs(colour = "Likelihood", x="Simulation", y="Beta_4 Estimate", title="Simulated Posterior Estimates for Beta_4")
```

